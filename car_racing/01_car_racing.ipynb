{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a16d3a2b4b78bd0c8a054524d667d1c",
     "grade": false,
     "grade_id": "cell-3a093c227c1a8513",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from rl_glue import RLGlue\n",
    "from environment import BaseEnvironment\n",
    "from car_racing import CarRacingEnvironment\n",
    "from agent import BaseAgent\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "import shutil\n",
    "from plot_script import plot_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "# Initialize speed\n",
    "all_speeds = np.load('speeds.npy', allow_pickle=True)\n",
    "\n",
    "# Initialize last speed\n",
    "last_speed = all_speeds[0]\n",
    "\n",
    "speed_dict = [0]\n",
    "speed_image_dict = [last_speed]\n",
    "\n",
    "for idx, sp in enumerate(all_speeds[1:]):\n",
    "    if not (sp == last_speed).all():\n",
    "        speed_dict.append(idx+1)\n",
    "        speed_image_dict.append(sp)\n",
    "\n",
    "        # Update Speed\n",
    "        last_speed = sp\n",
    "\n",
    "car_front = [65, 48]\n",
    "\n",
    "\n",
    "\n",
    "# Transform State\n",
    "def is_road(pixel):\n",
    "    if pixel[1]>pixel[0]*1.2 and pixel[1]>pixel[2]*1.2:\n",
    "        # Not grey (probably?)\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def calculate_speed(self, input_img):\n",
    "    speed_indicator = input_img[-12:,11:11+5,:]\n",
    "    for idx, sp_img in enumerate(self.speed_image_dict):\n",
    "\n",
    "        if (speed_indicator==sp_img).all():\n",
    "            return self.speed_dict[idx]\n",
    "\n",
    "    print(\"Error: Speed not found\")\n",
    "    return -1\n",
    "\n",
    "def transform_state(self, last_state):\n",
    "\n",
    "    #print(\"Input funcion\", last_state)\n",
    "\n",
    "    if len(last_state)!=4:\n",
    "        # Override state\n",
    "        speed = self.calculate_speed(last_state)\n",
    "\n",
    "        # Front Sensor\n",
    "        sensor_front = self.car_front[0]\n",
    "        for i in range(self.car_front[0]):\n",
    "            pixel = last_state[self.car_front[0]-i, self.car_front[1], :]\n",
    "            if not self.is_road(pixel):\n",
    "                sensor_front = i\n",
    "                break\n",
    "\n",
    "        # Left Sensor\n",
    "        sensor_left = self.car_front[0]\n",
    "        for i in range(self.car_front[0]):\n",
    "            pixel = last_state[self.car_front[0]-i, self.car_front[1], :]\n",
    "            if not self.is_road(pixel):\n",
    "                sensor_left = i\n",
    "                break\n",
    "\n",
    "        # Right Sensor\n",
    "        sensor_right = self.car_front[0]\n",
    "        for i in range(self.car_front[0]):\n",
    "            pixel = last_state[self.car_front[0]-i, self.car_front[1], :]\n",
    "            if not self.is_road(pixel):\n",
    "                sensor_right = i\n",
    "                break\n",
    "\n",
    "        return [speed, sensor_front, sensor_left, sensor_right]\n",
    "    else:\n",
    "        return last_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "833da49e040139194c4c5e7c68b23bee",
     "grade": false,
     "grade_id": "cell-c1f6c6471017fd99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Action-Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d10feeabf000214a0f53c5dfc5812437",
     "grade": false,
     "grade_id": "cell-e6d82e74c686dbf5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ActionValueNetwork:\n",
    "\n",
    "    def __init__(self, network_config):\n",
    "\n",
    "        self.state_dim = network_config.get(\"state_dim\")\n",
    "        self.num_hidden_units = network_config.get(\"num_hidden_units\")\n",
    "        self.num_actions = network_config.get(\"num_actions\")\n",
    "        \n",
    "        self.rand_generator = np.random.RandomState(network_config.get(\"seed\"))\n",
    "        \n",
    "        # NN Architecture\n",
    "        self.layer_sizes = [self.state_dim, self.num_hidden_units, self.num_actions]\n",
    "        \n",
    "        # Weights\n",
    "        self.weights = [dict() for i in range(0, len(self.layer_sizes) - 1)]\n",
    "        for i in range(0, len(self.layer_sizes) - 1):\n",
    "            self.weights[i]['W'] = self.init_saxe(self.layer_sizes[i], self.layer_sizes[i + 1])\n",
    "            self.weights[i]['b'] = np.zeros((1, self.layer_sizes[i + 1]))\n",
    "    \n",
    "    # Compute action-value\n",
    "    def get_action_values(self, s):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s (Numpy array): The state.\n",
    "        Returns:\n",
    "            The action-values (Numpy array) calculated using the network's weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        W0, b0 = self.weights[0]['W'], self.weights[0]['b']\n",
    "        psi = np.dot(s, W0) + b0\n",
    "        x = np.maximum(psi, 0) # Relu (Activation)\n",
    "        \n",
    "        W1, b1 = self.weights[1]['W'], self.weights[1]['b']\n",
    "        q_vals = np.dot(x, W1) + b1\n",
    "\n",
    "        return q_vals\n",
    "    \n",
    "    def get_TD_update(self, s, delta_mat):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s (Numpy array): The state.\n",
    "            delta_mat (Numpy array): A 2D array of shape (batch_size, num_actions). Each row of delta_mat  \n",
    "            correspond to one state in the batch. Each row has only one non-zero element \n",
    "            which is the TD-error corresponding to the action taken.\n",
    "        Returns:\n",
    "            The TD update (Array of dictionaries with gradient times TD errors) for the network's weights\n",
    "        \"\"\"\n",
    "\n",
    "        W0, b0 = self.weights[0]['W'], self.weights[0]['b']\n",
    "        W1, b1 = self.weights[1]['W'], self.weights[1]['b']\n",
    "        \n",
    "        psi = np.dot(s, W0) + b0\n",
    "        x = np.maximum(psi, 0)\n",
    "        dx = (psi > 0).astype(float)\n",
    "\n",
    "        # td_update has the same structure as self.weights, that is an array of dictionaries.\n",
    "        # td_update[0][\"W\"], td_update[0][\"b\"], td_update[1][\"W\"], and td_update[1][\"b\"] have the same shape as \n",
    "        # self.weights[0][\"W\"], self.weights[0][\"b\"], self.weights[1][\"W\"], and self.weights[1][\"b\"] respectively\n",
    "        td_update = [dict() for i in range(len(self.weights))]\n",
    "         \n",
    "        v = delta_mat\n",
    "        td_update[1]['W'] = np.dot(x.T, v) * 1. / s.shape[0]\n",
    "        td_update[1]['b'] = np.sum(v, axis=0, keepdims=True) * 1. / s.shape[0]\n",
    "        \n",
    "        v = np.dot(v, W1.T) * dx\n",
    "        td_update[0]['W'] = np.dot(s.T, v) * 1. / s.shape[0]\n",
    "        td_update[0]['b'] = np.sum(v, axis=0, keepdims=True) * 1. / s.shape[0]\n",
    "                \n",
    "        return td_update\n",
    "    \n",
    "    # Work Required: No. You may wish to read the relevant paper for more information on this weight initialization\n",
    "    # (Exact solutions to the nonlinear dynamics of learning in deep linear neural networks by Saxe, A et al., 2013)\n",
    "    def init_saxe(self, rows, cols):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rows (int): number of input units for layer.\n",
    "            cols (int): number of output units for layer.\n",
    "        Returns:\n",
    "            NumPy Array consisting of weights for the layer based on the initialization in Saxe et al.\n",
    "        \"\"\"\n",
    "        tensor = self.rand_generator.normal(0, 1, (rows, cols))\n",
    "        if rows < cols:\n",
    "            tensor = tensor.T\n",
    "        tensor, r = np.linalg.qr(tensor)\n",
    "        d = np.diag(r, 0)\n",
    "        ph = np.sign(d)\n",
    "        tensor *= ph\n",
    "\n",
    "        if rows < cols:\n",
    "            tensor = tensor.T\n",
    "        return tensor\n",
    "    \n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Returns: \n",
    "            A copy of the current weights of this network.\n",
    "        \"\"\"\n",
    "        return deepcopy(self.weights)\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            weights (list of dictionaries): Consists of weights that this network will set as its own weights.\n",
    "        \"\"\"\n",
    "        self.weights = deepcopy(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "567638b2dd6adb9971de931d9992b4e1",
     "grade": false,
     "grade_id": "cell-9020651e057104f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 2: Adam Optimizer\n",
    "\n",
    "In this assignment, you will use the Adam algorithm for updating the weights of your action-value network. As you may remember from Course 3 Assignment 2, the Adam algorithm is a more advanced variant of stochastic gradient descent (SGD). The Adam algorithm improves the SGD update with two concepts: adaptive vector stepsizes and momentum. It keeps running estimates of the mean and second moment of the updates, denoted by $\\mathbf{m}$ and $\\mathbf{v}$ respectively:\n",
    "$$\\mathbf{m_t} = \\beta_m \\mathbf{m_{t-1}} + (1 - \\beta_m)g_t \\\\\n",
    "\\mathbf{v_t} = \\beta_v \\mathbf{v_{t-1}} + (1 - \\beta_v)g^2_t\n",
    "$$\n",
    "\n",
    "Here, $\\beta_m$ and $\\beta_v$ are fixed parameters controlling the linear combinations above and $g_t$ is the update at time $t$ (generally the gradients, but here the TD error times the gradients).\n",
    "\n",
    "Given that $\\mathbf{m}$ and $\\mathbf{v}$ are initialized to zero, they are biased toward zero. To get unbiased estimates of the mean and second moment, Adam defines $\\mathbf{\\hat{m}}$ and $\\mathbf{\\hat{v}}$ as:\n",
    "$$ \\mathbf{\\hat{m}_t} = \\frac{\\mathbf{m_t}}{1 - \\beta_m^t} \\\\\n",
    "\\mathbf{\\hat{v}_t} = \\frac{\\mathbf{v_t}}{1 - \\beta_v^t}\n",
    "$$\n",
    "\n",
    "The weights are then updated as follows:\n",
    "$$ \\mathbf{w_t} = \\mathbf{w_{t-1}} + \\frac{\\alpha}{\\sqrt{\\mathbf{\\hat{v}_t}}+\\epsilon} \\mathbf{\\hat{m}_t}\n",
    "$$\n",
    "\n",
    "Here, $\\alpha$ is the step size parameter and $\\epsilon$ is another small parameter to keep the denominator from being zero.\n",
    "\n",
    "In the cell below, you will implement the `__init__()` and `update_weights()` methods for the Adam algorithm. In `__init__()`, you will initialize `self.m` and `self.v`. In `update_weights()`, you will compute new weights given the input weights and an update $g$ (here `td_errors_times_gradients`) according to the equations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "798d4618ba32342f63eb237947151a4a",
     "grade": false,
     "grade_id": "cell-585fd403a17cf660",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Adam():\n",
    " \n",
    "    def __init__(self, layer_sizes, optimizer_info):\n",
    "        \n",
    "        self.layer_sizes = layer_sizes\n",
    "\n",
    "        # Specify Adam algorithm's hyper parameters\n",
    "        self.step_size = optimizer_info.get(\"step_size\")\n",
    "        self.beta_m = optimizer_info.get(\"beta_m\")\n",
    "        self.beta_v = optimizer_info.get(\"beta_v\")\n",
    "        self.epsilon = optimizer_info.get(\"epsilon\")\n",
    "        \n",
    "        # Initialize Adam algorithm's m and v\n",
    "        self.m = [dict() for i in range(1, len(self.layer_sizes))]\n",
    "        self.v = [dict() for i in range(1, len(self.layer_sizes))]\n",
    "        \n",
    "        for i in range(0, len(self.layer_sizes) - 1):\n",
    "            # Inizialization\n",
    "            self.m[i][\"W\"] = np.zeros((self.layer_sizes[i],self.layer_sizes[i+1]))\n",
    "            self.m[i][\"b\"] = np.zeros( (1,self.layer_sizes[i+1]) )\n",
    "            self.v[i][\"W\"] = np.zeros((self.layer_sizes[i],self.layer_sizes[i+1]))\n",
    "            self.v[i][\"b\"] = np.zeros( (1,self.layer_sizes[i+1]) )\n",
    "            \n",
    "        self.beta_m_product = self.beta_m\n",
    "        self.beta_v_product = self.beta_v\n",
    "    \n",
    "\n",
    "    def update_weights(self, weights, td_errors_times_gradients):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            weights (Array of dictionaries): The weights of the neural network.\n",
    "            td_errors_times_gradients (Array of dictionaries): The gradient of the \n",
    "            action-values with respect to the network's weights times the TD-error\n",
    "        Returns:\n",
    "            The updated weights (Array of dictionaries).\n",
    "        \"\"\"\n",
    "        for i in range(len(weights)):\n",
    "            for param in weights[i].keys():\n",
    "\n",
    "                self.m[i][param] = (self.beta_m * self.m[i][param]) + (1-self.beta_m) * td_errors_times_gradients[i][param]\n",
    "                self.v[i][param] = (self.beta_v * self.v[i][param]) + (1-self.beta_v)*(td_errors_times_gradients[i][param]**2)\n",
    "                m_hat = self.m[i][param] / (1-self.beta_m_product)\n",
    "                v_hat = self.v[i][param] / (1-self.beta_v_product)\n",
    "                weight_update = (self.step_size * m_hat) / (v_hat**0.5 + self.epsilon )\n",
    "  \n",
    "                weights[i][param] = weights[i][param] + weight_update\n",
    "\n",
    "        self.beta_m_product *= self.beta_m\n",
    "        self.beta_v_product *= self.beta_v\n",
    "        \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd216bf2169746f6331d6a5fbd79d605",
     "grade": false,
     "grade_id": "cell-1e1aaa0d442eb015",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size, minibatch_size, seed):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size (integer): The size of the replay buffer.              \n",
    "            minibatch_size (integer): The sample size.\n",
    "            seed (integer): The seed for the random number generator. \n",
    "        \"\"\"\n",
    "        self.buffer = []\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.rand_generator = np.random.RandomState(seed)\n",
    "        self.max_size = size\n",
    "\n",
    "    def append(self, state, action, reward, terminal, next_state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): The state.              \n",
    "            action (integer): The action.\n",
    "            reward (float): The reward.\n",
    "            terminal (integer): 1 if the next state is a terminal state and 0 otherwise.\n",
    "            next_state (Numpy array): The next state.           \n",
    "        \"\"\"\n",
    "        if len(self.buffer) == self.max_size:\n",
    "            del self.buffer[0]\n",
    "        self.buffer.append([state, action, reward, terminal, next_state])\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            A list of transition tuples including state, action, reward, terinal, and next_state\n",
    "        \"\"\"\n",
    "        idxs = self.rand_generator.choice(np.arange(len(self.buffer)), size=self.minibatch_size)\n",
    "        return [self.buffer[idx] for idx in idxs]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0bc082ff0d5b933fb88fa1936f2057d3",
     "grade": false,
     "grade_id": "cell-b32ebbeb60c5b9f7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax(action_values, tau=1.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        action_values (Numpy array): A 2D array of shape (batch_size, num_actions). \n",
    "                       The action-values computed by an action-value network.              \n",
    "        tau (float): The temperature parameter scalar.\n",
    "    Returns:\n",
    "        A 2D array of shape (batch_size, num_actions). Where each column is a probability distribution over\n",
    "        the actions representing the policy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the preferences by dividing the action-values by the temperature parameter tau\n",
    "    preferences = action_values / tau\n",
    "    # Compute the maximum preference across the actions\n",
    "    max_preference = np.max(preferences, axis=1)\n",
    "\n",
    "    # Reshape max_preference array which has shape [Batch,] to [Batch, 1]. This allows NumPy broadcasting \n",
    "    # when subtracting the maximum preference from the preference of each action.\n",
    "    reshaped_max_preference = max_preference.reshape((-1, 1))\n",
    "    \n",
    "    # Compute the numerator, i.e., the exponential of the preference - the max preference.\n",
    "    exp_preferences = np.exp(preferences - reshaped_max_preference)\n",
    "    # Compute the denominator, i.e., the sum over the numerator along the actions axis.\n",
    "    sum_of_exp_preferences = np.sum(exp_preferences,axis=1)\n",
    "    \n",
    "\n",
    "    # Reshape sum_of_exp_preferences array which has shape [Batch,] to [Batch, 1] to  allow for NumPy broadcasting \n",
    "    # when dividing the numerator by the denominator.\n",
    "    reshaped_sum_of_exp_preferences = sum_of_exp_preferences.reshape((-1, 1))\n",
    "    \n",
    "    # Compute the action probabilities according to the equation in the previous cell.\n",
    "    action_probs = exp_preferences / reshaped_sum_of_exp_preferences\n",
    "\n",
    "    # squeeze() removes any singleton dimensions. It is used here because this function is used in the \n",
    "    # agent policy when selecting an action (for which the batch dimension is 1.) As np.random.choice is used in \n",
    "    # the agent policy and it expects 1D arrays, we need to remove this singleton batch dimension.\n",
    "    action_probs = action_probs.squeeze()\n",
    "    return action_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b74fe561e81cd10e366c2ad76673248",
     "grade": false,
     "grade_id": "cell-9d3660107222383d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Putting the pieces together\n",
    "\n",
    "In this section, you will combine components from the previous sections to write up an RL-Glue Agent. The main component that you will implement is the action-value network updates with experience sampled from the experience replay buffer.\n",
    "\n",
    "At time $t$, we have an action-value function represented as a neural network, say $Q_t$. We want to update our action-value function and get a new one we can use at the next timestep. We will get this $Q_{t+1}$ using multiple replay steps that each result in an intermediate action-value function $Q_{t+1}^{i}$ where $i$ indexes which replay step we are at.\n",
    "\n",
    "In each replay step, we sample a batch of experiences from the replay buffer and compute a minibatch Expected-SARSA update. Across these N replay steps, we will use the current \"un-updated\" action-value network at time $t$, $Q_t$, for computing the action-values of the next-states. This contrasts using the most recent action-values from the last replay step $Q_{t+1}^{i}$. We make this choice to have targets that are stable across replay steps. Here is the pseudocode for performing the updates:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& Q_t \\leftarrow \\text{action-value network at timestep t (current action-value network)}\\\\\n",
    "& \\text{Initialize } Q_{t+1}^1 \\leftarrow Q_t\\\\\n",
    "& \\text{For } i \\text{ in } [1, ..., N] \\text{ (i.e. N} \\text{  replay steps)}:\\\\\n",
    "& \\hspace{1cm} s, a, r, t, s'\n",
    "\\leftarrow \\text{Sample batch of experiences from experience replay buffer} \\\\\n",
    "& \\hspace{1cm} \\text{Do Expected Sarsa update with } Q_t: Q_{t+1}^{i+1}(s, a) \\leftarrow Q_{t+1}^{i}(s, a) + \\alpha \\cdot \\left[r + \\gamma \\left(\\sum_{b} \\pi(b | s') Q_t(s', b)\\right) - Q_{t+1}^{i}(s, a)\\right]\\\\\n",
    "& \\hspace{1.5cm} \\text{ making sure to add the } \\gamma \\left(\\sum_{b} \\pi(b | s') Q_t(s', b)\\right) \\text{ for non-terminal transitions only.} \\\\\n",
    "& \\text{After N replay steps, we set } Q_{t+1}^{N} \\text{ as } Q_{t+1} \\text{ and have a new } Q_{t+1} \\text{for time step } t + 1 \\text{ that we will fix in the next set of updates. }\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As you can see in the pseudocode, after sampling a batch of experiences, we do many computations. The basic idea however is that we are looking to compute a form of a TD error. In order to so, we can take the following steps:\n",
    "- compute the action-values for the next states using the action-value network $Q_{t}$,\n",
    "- compute the policy $\\pi(b | s')$ induced by the action-values $Q_{t}$ (using the softmax function you implemented before),\n",
    "- compute the Expected sarsa targets $r + \\gamma \\left(\\sum_{b} \\pi(b | s') Q_t(s', b)\\right)$,\n",
    "- compute the action-values for the current states using the latest $Q_{t + 1}$, and,\n",
    "- compute the TD-errors with the Expected Sarsa targets.\n",
    " \n",
    "For the third step above, you can start by computing $\\pi(b | s') Q_t(s', b)$ followed by summation to get $\\hat{v}_\\pi(s') = \\left(\\sum_{b} \\pi(b | s') Q_t(s', b)\\right)$. $\\hat{v}_\\pi(s')$ is an estimate of the value of the next state. Note for terminal next states, $\\hat{v}_\\pi(s') = 0$. Finally, we add the rewards to the discount times $\\hat{v}_\\pi(s')$.\n",
    "\n",
    "You will implement these steps in the `get_td_error()` function below which given a batch of experiences (including states, next_states, actions, rewards, terminals), fixed action-value network (current_q), and action-value network (network), computes the TD error in the form of a 1D array of size batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e8ff7f160ca26a7639acc062ae6b29a",
     "grade": false,
     "grade_id": "cell-f370691c828efad9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Work Required: Yes. Fill in code in get_td_error (~9 Lines).\n",
    "def get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        states (Numpy array): The batch of states with the shape (batch_size, state_dim).\n",
    "        next_states (Numpy array): The batch of next states with the shape (batch_size, state_dim).\n",
    "        actions (Numpy array): The batch of actions with the shape (batch_size,).\n",
    "        rewards (Numpy array): The batch of rewards with the shape (batch_size,).\n",
    "        discount (float): The discount factor.\n",
    "        terminals (Numpy array): The batch of terminals with the shape (batch_size,).\n",
    "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
    "        current_q (ActionValueNetwork): The fixed network used for computing the targets, \n",
    "                                        and particularly, the action-values at the next-states.\n",
    "    Returns:\n",
    "        The TD errors (Numpy array) for actions taken, of shape (batch_size,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Note: Here network is the latest state of the network that is getting replay updates. In other words, \n",
    "    # the network represents Q_{t+1}^{i} whereas current_q represents Q_t, the fixed network used for computing the \n",
    "    # targets, and particularly, the action-values at the next-states.\n",
    "    \n",
    "    # Compute action values at next states using current_q network\n",
    "    # Note that q_next_mat is a 2D array of shape (batch_size, num_actions)\n",
    "    \n",
    "    ### START CODE HERE (~1 Line)\n",
    "    q_next_mat = current_q.get_action_values(next_states)\n",
    "    ### END CODE HERE\n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    # Compute policy at next state by passing the action-values in q_next_mat to softmax()\n",
    "    # Note that probs_mat is a 2D array of shape (batch_size, num_actions)\n",
    "    \n",
    "    ### START CODE HERE (~1 Line)\n",
    "    probs_mat = softmax(q_next_mat,tau)\n",
    "    ### END CODE HERE\n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    # Compute the estimate of the next state value, v_next_vec.\n",
    "    # Hint: sum the action-values for the next_states weighted by the policy, probs_mat. Then, multiply by\n",
    "    # (1 - terminals) to make sure v_next_vec is zero for terminal next states.\n",
    "    # Note that v_next_vec is a 1D array of shape (batch_size,)\n",
    "    \n",
    "    ### START CODE HERE (~3 Lines)\n",
    "    weighted_next_mat = q_next_mat*probs_mat\n",
    "    action_values_sum = np.sum(weighted_next_mat,axis=1)\n",
    "    v_next_vec = action_values_sum*(1-terminals)\n",
    "    ### END CODE HERE\n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    # Compute Expected Sarsa target\n",
    "    # Note that target_vec is a 1D array of shape (batch_size,)\n",
    "    \n",
    "    ### START CODE HERE (~1 Line)\n",
    "    target_vec = rewards + discount*v_next_vec\n",
    "    ### END CODE HERE\n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    # Compute action values at the current states for all actions using network\n",
    "    # Note that q_mat is a 2D array of shape (batch_size, num_actions)\n",
    "    \n",
    "    ### START CODE HERE (~1 Line)\n",
    "    q_mat = network.get_action_values(states)\n",
    "    ### END CODE HERE\n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    # Batch Indices is an array from 0 to the batch size - 1. \n",
    "    batch_indices = np.arange(q_mat.shape[0])\n",
    "\n",
    "    # Compute q_vec by selecting q(s, a) from q_mat for taken actions\n",
    "    # Use batch_indices as the index for the first dimension of q_mat\n",
    "    # Note that q_vec is a 1D array of shape (batch_size)\n",
    "    \n",
    "    ### START CODE HERE (~1 Line)\n",
    "    q_vec = q_mat[batch_indices,actions]\n",
    "    ### END CODE HERE\n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    # Compute TD errors for actions taken\n",
    "    # Note that delta_vec is a 1D array of shape (batch_size)\n",
    "    \n",
    "    ### START CODE HERE (~1 Line)\n",
    "    delta_vec = target_vec - q_vec\n",
    "    ### END CODE HERE\n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    return delta_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_network(experiences, discount, optimizer, network, current_q, tau):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        experiences (Numpy array): The batch of experiences including the states, actions, \n",
    "                                   rewards, terminals, and next_states.\n",
    "        discount (float): The discount factor.\n",
    "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
    "        current_q (ActionValueNetwork): The fixed network used for computing the targets, \n",
    "                                        and particularly, the action-values at the next-states.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get states, action, rewards, terminals, and next_states from experiences\n",
    "    states, actions, rewards, terminals, next_states = map(list, zip(*experiences))\n",
    "    states = np.concatenate(states)\n",
    "    next_states = np.concatenate(next_states)\n",
    "    rewards = np.array(rewards)\n",
    "    terminals = np.array(terminals)\n",
    "    batch_size = states.shape[0]\n",
    "\n",
    "    # Compute TD error using the get_td_error function\n",
    "    # Note that q_vec is a 1D array of shape (batch_size)\n",
    "    delta_vec = get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau)\n",
    "\n",
    "    # Batch Indices is an array from 0 to the batch_size - 1. \n",
    "    batch_indices = np.arange(batch_size)\n",
    "\n",
    "    # Make a td error matrix of shape (batch_size, num_actions)\n",
    "    # delta_mat has non-zero value only for actions taken\n",
    "    delta_mat = np.zeros((batch_size, network.num_actions))\n",
    "    delta_mat[batch_indices, actions] = delta_vec\n",
    "\n",
    "    # Pass delta_mat to compute the TD errors times the gradients of the network's weights from back-propagation\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    td_update = network.get_TD_update(states,delta_mat)\n",
    "    ### END CODE HERE\n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    # Pass network.get_weights and the td_update to the optimizer to get updated weights\n",
    "    ### START CODE HERE\n",
    "    weights = optimizer.update_weights(network.get_weights(), td_update)\n",
    "    ### END CODE HERE\n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    network.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(BaseAgent):\n",
    "    def __init__(self):\n",
    "        self.name = \"expected_sarsa_agent\"\n",
    "        \n",
    "    # Work Required: No.\n",
    "    def agent_init(self, agent_config):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "\n",
    "        Set parameters needed to setup the agent.\n",
    "\n",
    "        Assume agent_config dict contains:\n",
    "        {\n",
    "            network_config: dictionary,\n",
    "            optimizer_config: dictionary,\n",
    "            replay_buffer_size: integer,\n",
    "            minibatch_sz: integer, \n",
    "            num_replay_updates_per_step: float\n",
    "            discount_factor: float,\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.replay_buffer = ReplayBuffer(agent_config['replay_buffer_size'], \n",
    "                                          agent_config['minibatch_sz'], agent_config.get(\"seed\"))\n",
    "        self.network = ActionValueNetwork(agent_config['network_config'])\n",
    "        self.optimizer = Adam(self.network.layer_sizes, agent_config[\"optimizer_config\"])\n",
    "        self.num_actions = agent_config['network_config']['num_actions']\n",
    "        self.num_replay = agent_config['num_replay_updates_per_step']\n",
    "        self.discount = agent_config['gamma']\n",
    "        self.tau = agent_config['tau']\n",
    "        \n",
    "        self.rand_generator = np.random.RandomState(agent_config.get(\"seed\"))\n",
    "        \n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        \n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "\n",
    "    # Work Required: No.\n",
    "    def policy(self, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): the state.\n",
    "        Returns:\n",
    "            the action. \n",
    "        \"\"\"\n",
    "        action_values = self.network.get_action_values(state)\n",
    "        probs_batch = softmax(action_values, self.tau)\n",
    "        action = self.rand_generator.choice(self.num_actions, p=probs_batch.squeeze())\n",
    "        return action\n",
    "\n",
    "    # Work Required: No.\n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            The first action the agent takes.\n",
    "        \"\"\"\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.last_state = np.array([state])\n",
    "        self.last_action = self.policy(self.last_state)\n",
    "        return self.last_action\n",
    "\n",
    "    # Work Required: Yes. Fill in the action selection, replay-buffer update, \n",
    "    # weights update using optimize_network, and updating last_state and last_action (~5 lines).\n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "\n",
    "        # Make state an array of shape (1, state_dim) to add a batch dimension and\n",
    "        # to later match the get_action_values() and get_TD_update() functions\n",
    "        state = np.array([state])\n",
    "\n",
    "        # Select action\n",
    "        action = self.policy(state)\n",
    "\n",
    "        # Append new experience to replay buffer\n",
    "        # Note: look at the replay_buffer append function for the order of arguments\n",
    "\n",
    "        # your code here\n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, 0, state)\n",
    "        \n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            current_q = deepcopy(self.network)\n",
    "            for _ in range(self.num_replay):\n",
    "                \n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                \n",
    "                # Call optimize_network to update the weights of the network (~1 Line)\n",
    "                # your code here\n",
    "                optimize_network(experiences,self.discount,self.optimizer,self.network,current_q,self.tau)\n",
    "                \n",
    "        # Update the last state and last action.\n",
    "        ### START CODE HERE (~2 Lines)\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        ### END CODE HERE\n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "        return action\n",
    "\n",
    "    # Work Required: Yes. Fill in the replay-buffer update and\n",
    "    # update of the weights using optimize_network (~2 lines).\n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        \n",
    "        # Set terminal state to an array of zeros\n",
    "        state = np.zeros_like(self.last_state)\n",
    "\n",
    "        # Append new experience to replay buffer\n",
    "        # Note: look at the replay_buffer append function for the order of arguments\n",
    "        \n",
    "        # your code here\n",
    "        self.replay_buffer.append(self.last_state,self.last_action,reward,1,state)\n",
    "        \n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            current_q = deepcopy(self.network)\n",
    "            for _ in range(self.num_replay):\n",
    "                \n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                \n",
    "                # Call optimize_network to update the weights of the network\n",
    "                # your code here\n",
    "                optimize_network(experiences,self.discount,self.optimizer,self.network,current_q,self.tau)\n",
    "                \n",
    "        \n",
    "    def agent_message(self, message):\n",
    "        if message == \"get_sum_reward\":\n",
    "            return self.sum_rewards\n",
    "        else:\n",
    "            raise Exception(\"Unrecognized Message!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ed9fca0352062809443beae983d9ea2",
     "grade": false,
     "grade_id": "cell-83130c3c2426b0c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 6: Run Experiment\n",
    "\n",
    "Now that you implemented the agent, we can use it to run an experiment on the Lunar Lander problem. We will plot the learning curve of the agent to visualize learning progress. To plot the learning curve, we use the sum of rewards in an episode as the performance measure. We have provided for you the experiment/plot code in the cell below which you can go ahead and run. Note that running the cell below has taken approximately 10 minutes in prior testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e192cd7f474ff57861f6f8a3e3ab188c",
     "grade": false,
     "grade_id": "cell-0defecc3f69370dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carlo\\Envs\\RL\\lib\\site-packages\\gym\\core.py:200: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n",
      "  5%|▌         | 1/20 [00:07<02:20,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.27868338557974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:14<02:08,  7.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.74412811387992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:21<02:02,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.883281733746173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:28<01:55,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.09999999999927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:36<01:49,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119.25309446254346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:43<01:42,  7.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270.89847908744736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:50<01:34,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133.0824561403535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:58<01:27,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.62396166134357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [01:05<01:19,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.93146067416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [01:12<01:13,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234.00681003584486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [01:16<00:55,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.500000000000838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [01:23<00:52,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.22499999999923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [01:31<00:47,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115.96206896551996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [01:38<00:42,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.88378378378627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [01:46<00:35,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.57540983606485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [01:53<00:28,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321.2320754716915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [02:00<00:21,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.741509433962904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [02:08<00:14,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.276470588234705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [02:14<00:06,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.4695501730124505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:21<00:00,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.629209621994505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters):\n",
    "    \n",
    "    rl_glue = RLGlue(environment, agent)\n",
    "        \n",
    "    # save sum of reward at the end of each episode\n",
    "    agent_sum_reward = np.zeros((experiment_parameters[\"num_runs\"], \n",
    "                                 experiment_parameters[\"num_episodes\"]))\n",
    "\n",
    "    env_info = {}\n",
    "\n",
    "    agent_info = agent_parameters\n",
    "\n",
    "    # one agent setting\n",
    "    for run in range(1, experiment_parameters[\"num_runs\"]+1):\n",
    "        agent_info[\"seed\"] = run\n",
    "        agent_info[\"network_config\"][\"seed\"] = run\n",
    "        env_info[\"seed\"] = run\n",
    "\n",
    "        rl_glue.rl_init(agent_info, env_info)\n",
    "        \n",
    "        for episode in tqdm(range(1, experiment_parameters[\"num_episodes\"]+1)):\n",
    "            # run episode\n",
    "            rl_glue.rl_episode(experiment_parameters[\"timeout\"])\n",
    "            \n",
    "            episode_reward = rl_glue.rl_agent_message(\"get_sum_reward\")\n",
    "            agent_sum_reward[run - 1, episode - 1] = episode_reward\n",
    "            print(episode_reward)\n",
    "    save_name = \"{}\".format(rl_glue.agent.name)\n",
    "    \n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "    np.save(\"results/sum_reward_{}\".format(save_name), agent_sum_reward)\n",
    "    shutil.make_archive('results', 'zip', 'results')\n",
    "\n",
    "    return rl_glue # To get the agent\n",
    "\n",
    "# Run Experiment\n",
    "\n",
    "# Experiment parameters\n",
    "experiment_parameters = {\n",
    "    \"num_runs\" : 1,\n",
    "    \"num_episodes\" : 5,\n",
    "    \"timeout\" : 600, # timestep limit (set to 500 as default)\n",
    "}\n",
    "\n",
    "# Environment parameters\n",
    "environment_parameters = {}\n",
    "\n",
    "current_env = CarRacingEnvironment\n",
    "\n",
    "# Agent parameters\n",
    "agent_parameters = {\n",
    "    'network_config': {\n",
    "        'state_dim': 6, # State Dimension (number of inputs)\n",
    "        'num_hidden_units': 16, #64OP\n",
    "        'num_actions': 5 # Action Space (number of outputs)\n",
    "    },\n",
    "    'optimizer_config': {\n",
    "        'step_size': 1e-3, # 1e-3\n",
    "        'beta_m': 0.9, \n",
    "        'beta_v': 0.999, # 0.999\n",
    "        'epsilon': 1e-8\n",
    "    },\n",
    "    'replay_buffer_size': 50000,\n",
    "    'minibatch_sz': 8,\n",
    "    'num_replay_updates_per_step': 4,\n",
    "    'gamma': 0.99,\n",
    "    'tau': 0.001\n",
    "}\n",
    "current_agent = Agent\n",
    "\n",
    "# run experiment\n",
    "rl_glue_return = run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92ba982f59ab0ecd45333f5b73f0be60",
     "grade": false,
     "grade_id": "cell-b6321a32b126637e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the cell below to see the comparison between the agent that you implemented and a random agent for the one run and 300 episodes. Note that the `plot_result()` function smoothes the learning curve by applying a sliding window on the performance measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3132510fde7c06020276a6c6f272eccd",
     "grade": false,
     "grade_id": "cell-337be142123eb81f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_result([\"expected_sarsa_agent\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Car Racing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"CarRacing-v1\", continuous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "1999\n"
     ]
    }
   ],
   "source": [
    "# All at once\n",
    "observation, info = env.reset(return_info=True)\n",
    "for i in range(2400):\n",
    "   env.render()\n",
    "   observation = rl_glue_return.transform_state(observation)\n",
    "   action = rl_glue_return.agent.policy(observation)\n",
    "   #print(observation)\n",
    "   observation, reward, done, info = env.step(action)\n",
    "   #print(action, reward)\n",
    "\n",
    "   if done:\n",
    "      observation, info = env.reset(return_info=True)\n",
    "      print(i)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step By step\n",
    "observation, info = env.reset(return_info=True)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38.91318592273405, 0, 0, 0, 0, 0]\n",
      "3 -0.09999999999999432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 789,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step By STep\n",
    "\n",
    "observation = rl_glue_return.transform_state(observation)\n",
    "action = rl_glue_return.agent.policy(observation)\n",
    "#action = 3\n",
    "\n",
    "print(observation)\n",
    "\n",
    "observation, reward, done, info = env.step(action)\n",
    "print(action, reward)\n",
    "\n",
    "if done:\n",
    "    observation, info = env.reset(return_info=True)\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.10796227647534"
      ]
     },
     "execution_count": 790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.square(env.car.hull.linearVelocity[0]) + np.square(env.car.hull.linearVelocity[1]))"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "complete-reinforcement-learning-system",
   "graded_item_id": "8dMlx",
   "launcher_item_id": "4O5gG"
  },
  "interpreter": {
   "hash": "41c74caa6da4fd46948aae98c51b7a133fd5c80975207b27cf3047040559b6b4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
