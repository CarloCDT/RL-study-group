{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a16d3a2b4b78bd0c8a054524d667d1c",
     "grade": false,
     "grade_id": "cell-3a093c227c1a8513",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from rl_glue import RLGlue\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import shutil\n",
    "from plot_script import plot_result\n",
    "\n",
    "# Agent\n",
    "from agent import ExpectedSarsaAgent\n",
    "\n",
    "# Environment\n",
    "from car_racing import CarRacingEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ed9fca0352062809443beae983d9ea2",
     "grade": false,
     "grade_id": "cell-83130c3c2426b0c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e192cd7f474ff57861f6f8a3e3ab188c",
     "grade": false,
     "grade_id": "cell-0defecc3f69370dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:26<00:00, 13.20s/it]\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters):\n",
    "    \n",
    "    rl_glue = RLGlue(environment, agent)\n",
    "        \n",
    "    # save sum of reward at the end of each episode\n",
    "    agent_sum_reward = np.zeros((experiment_parameters[\"num_runs\"], \n",
    "                                 experiment_parameters[\"num_episodes\"]))\n",
    "\n",
    "    env_info = {}\n",
    "\n",
    "    agent_info = agent_parameters\n",
    "\n",
    "    # one agent setting\n",
    "    for run in range(1, experiment_parameters[\"num_runs\"]+1):\n",
    "        agent_info[\"seed\"] = run\n",
    "        agent_info[\"network_config\"][\"seed\"] = run\n",
    "        env_info[\"seed\"] = run\n",
    "\n",
    "        rl_glue.rl_init(agent_info, env_info)\n",
    "        \n",
    "        # Start from pre trained weights!\n",
    "        #rl_glue.agent.network.weights = np.load('trained_networks/agent_004_albon.npy', allow_pickle=True)\n",
    "\n",
    "        for episode in tqdm(range(1, experiment_parameters[\"num_episodes\"]+1)):\n",
    "            # run episode\n",
    "            rl_glue.rl_episode(experiment_parameters[\"timeout\"])\n",
    "            \n",
    "            episode_reward = rl_glue.rl_agent_message(\"get_sum_reward\")\n",
    "            agent_sum_reward[run - 1, episode - 1] = episode_reward\n",
    "            #print(episode_reward)\n",
    "\n",
    "    # Save Results\n",
    "            save_name = \"{}\".format(rl_glue.agent.name)\n",
    "            \n",
    "            if not os.path.exists('results'):\n",
    "                os.makedirs('results')\n",
    "            #np.save(\"results/sum_reward_{}\".format(save_name), agent_sum_reward)\n",
    "            np.save(\"results/sum_reward_{}\".format(save_name), agent_sum_reward[:,:episode])\n",
    "            shutil.make_archive('results', 'zip', 'results')\n",
    "\n",
    "    return rl_glue # To get the agent\n",
    "\n",
    "# Run Experiment\n",
    "\n",
    "# Experiment parameters\n",
    "experiment_parameters = {\n",
    "    \"num_runs\" : 1,\n",
    "    \"num_episodes\" : 2,\n",
    "    \"timeout\" : 1000, # timestep limit (set to 500 as default)\n",
    "}\n",
    "\n",
    "# Environment parameters\n",
    "environment_parameters = {}\n",
    "\n",
    "current_env = CarRacingEnvironment\n",
    "\n",
    "# Agent parameters\n",
    "agent_parameters = {\n",
    "    'network_config': {\n",
    "        'state_dim': 6, # State Dimension (number of inputs)\n",
    "        'num_hidden_units_1': 16, # num_hidden_units_1\n",
    "        #'num_hidden_units_2': 8, # dev\n",
    "        'num_actions': 5 # Action Space (number of outputs)\n",
    "    },\n",
    "    'optimizer_config': {\n",
    "        'step_size': 1e-3, # 1e-3\n",
    "        'beta_m': 0.9,\n",
    "        'beta_v': 0.999, # 0.999\n",
    "        'epsilon': 1e-8\n",
    "    },\n",
    "    'replay_buffer_size': 50000,\n",
    "    'minibatch_sz': 8,\n",
    "    'num_replay_updates_per_step': 2,\n",
    "    'gamma': 0.98,\n",
    "    'tau': 0.001 # 0.001\n",
    "}\n",
    "\n",
    "current_agent = ExpectedSarsaAgent\n",
    "\n",
    "# run experiment\n",
    "rl_glue_return = run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save/Load agent\n",
    "# np.save('trained_networks/agent_005.npy', rl_glue_return.agent.network.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3132510fde7c06020276a6c6f272eccd",
     "grade": false,
     "grade_id": "cell-337be142123eb81f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot_result([\"expected_sarsa_agent\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Car Racing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_glue_return.environment.env_init()\n",
    "rl_glue_return.environment.rand_generator = np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Environment\n",
    "observation = rl_glue_return.environment.env_start()\n",
    "transformed_obs = observation = rl_glue_return.transform_state(observation)\n",
    "transformed_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Action\n",
    "action = rl_glue_return.agent.policy(transformed_obs)\n",
    "print(f\"Action: {action}\")\n",
    "\n",
    "# Do Action\n",
    "reward, observation, is_terminal = rl_glue_return.environment.env_step(action)\n",
    "transformed_obs = transformed_obs = observation = rl_glue_return.transform_state(observation)\n",
    "print(f\"Transformed State: {transformed_obs}\")\n",
    "\n",
    "if is_terminal:\n",
    "    print(\"Terminal State\")\n",
    "    observation = rl_glue_return.environment.env_start()\n",
    "\n",
    "rl_glue_return.environment.env.render();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let it run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NN\n",
    "trained_weights = np.load('trained_networks/agent_003.npy', allow_pickle=True)\n",
    "rl_glue_return.agent.network.weights = trained_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 780/1000 [00:18<00:05, 41.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminal State: All tiles visited!\n",
      "Reward: 921.8999999999944\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start Environment\n",
    "rl_glue_return.environment.env_init()\n",
    "rl_glue_return.environment.rand_generator = np.random.RandomState(21) # 2, 4, 9, 21\n",
    "\n",
    "# Change tau (if needed)\n",
    "rl_glue_return.agent.tau = 0.0001\n",
    "time_steps = 1000\n",
    "\n",
    "observation = rl_glue_return.environment.env_start()\n",
    "transformed_obs = observation = rl_glue_return.transform_state(observation)\n",
    "episode_reward = 0\n",
    "\n",
    "for i in tqdm(range(time_steps)):\n",
    "    \n",
    "    # Select Action\n",
    "    action = rl_glue_return.agent.policy(transformed_obs)\n",
    "\n",
    "    # Do Action\n",
    "    reward, observation, is_terminal = rl_glue_return.environment.env_step(action)\n",
    "    transformed_obs = transformed_obs = observation = rl_glue_return.transform_state(observation)\n",
    "\n",
    "    episode_reward += reward\n",
    "\n",
    "    if is_terminal:\n",
    "        message = \"Terminal State: Max number of steps completed.\"\n",
    "        if i != time_steps-1:\n",
    "            message = \"Terminal State: All tiles visited!\"\n",
    "        print(message)\n",
    "        print(f'Reward: {episode_reward}')\n",
    "        break\n",
    "        observation = rl_glue_return.environment.env_start()\n",
    "\n",
    "    # Render\n",
    "    rl_glue_return.environment.env.render();\n",
    "\n",
    "rl_glue_return.environment.env.close();"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "complete-reinforcement-learning-system",
   "graded_item_id": "8dMlx",
   "launcher_item_id": "4O5gG"
  },
  "interpreter": {
   "hash": "41c74caa6da4fd46948aae98c51b7a133fd5c80975207b27cf3047040559b6b4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
